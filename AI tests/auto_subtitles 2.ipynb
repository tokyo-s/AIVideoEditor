{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ffmpeg?????????????? ffmpeg-python lib?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy\n",
    "# ! pip install dtw-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install git+https://github.com/linto-ai/whisper-timestamped\n",
    "# !pip install --upgrade --no-deps --force-reinstall git+https://github.com/linto-ai/whisper-timestamped #### to update lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\whisper\\transcribe.py:77: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21202/21202 [00:13<00:00, 1579.94frames/s]\n"
     ]
    }
   ],
   "source": [
    "# import whisper\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "from dtw import dtw\n",
    "from scipy.ndimage import median_filter\n",
    "import numpy as np\n",
    "\n",
    "import whisper_timestamped #as whisper\n",
    "\n",
    "MODEL_WHISPER_SIZE = \"tiny\" #large\n",
    "audio_file = \"data/test.mp3\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "model = whisper_timestamped.load_model(MODEL_WHISPER_SIZE, device=DEVICE)\n",
    "\n",
    "\n",
    "result = whisper_timestamped.transcribe(audio=audio_file, model=model)\n",
    "# def audio_to_text(audio):\n",
    "#     audio = whisper_timestamped.load_audio(audio)\n",
    "#     # audio = whisper.pad_or_trim(audio)\n",
    "#     # mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "#     # _, probs = model.detect_language(mel)\n",
    "\n",
    "#     # detected_language = max(probs, key=probs.get)\n",
    "#     result = model.transcribe(audio, language=\"en\")\n",
    "\n",
    "#     text = result[\"text\"]\n",
    "#     return text, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"I'm\", 'start': 0.02, 'end': 5.88, 'confidence': 0.126},\n",
       " {'text': 'gonna', 'start': 5.88, 'end': 5.92, 'confidence': 0.105},\n",
       " {'text': 'be', 'start': 5.92, 'end': 5.96, 'confidence': 0.132},\n",
       " {'text': 'the', 'start': 5.96, 'end': 6.96, 'confidence': 0.057},\n",
       " {'text': 'best', 'start': 6.96, 'end': 7.0, 'confidence': 0.098},\n",
       " {'text': 'of', 'start': 7.0, 'end': 7.78, 'confidence': 0.074},\n",
       " {'text': 'all.', 'start': 7.78, 'end': 7.82, 'confidence': 0.147}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['segments'][0]['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {\"af_za\": \"Afrikaans\", \"am_et\": \"Amharic\", \"ar_eg\": \"Arabic\", \"as_in\": \"Assamese\", \"az_az\": \"Azerbaijani\", \"be_by\": \"Belarusian\", \"bg_bg\": \"Bulgarian\", \"bn_in\": \"Bengali\", \"bs_ba\": \"Bosnian\", \"ca_es\": \"Catalan\", \"cmn_hans_cn\": \"Chinese\", \"cs_cz\": \"Czech\", \"cy_gb\": \"Welsh\", \"da_dk\": \"Danish\", \"de_de\": \"German\", \"el_gr\": \"Greek\", \"en_us\": \"English\", \"es_419\": \"Spanish\", \"et_ee\": \"Estonian\", \"fa_ir\": \"Persian\", \"fi_fi\": \"Finnish\", \"fil_ph\": \"Tagalog\", \"fr_fr\": \"French\", \"gl_es\": \"Galician\", \"gu_in\": \"Gujarati\", \"ha_ng\": \"Hausa\", \"he_il\": \"Hebrew\", \"hi_in\": \"Hindi\", \"hr_hr\": \"Croatian\", \"hu_hu\": \"Hungarian\", \"hy_am\": \"Armenian\", \"id_id\": \"Indonesian\", \"is_is\": \"Icelandic\", \"it_it\": \"Italian\", \"ja_jp\": \"Japanese\", \"jv_id\": \"Javanese\", \"ka_ge\": \"Georgian\", \"kk_kz\": \"Kazakh\", \"km_kh\": \"Khmer\", \"kn_in\": \"Kannada\", \"ko_kr\": \"Korean\", \"lb_lu\": \"Luxembourgish\", \"ln_cd\": \"Lingala\", \"lo_la\": \"Lao\", \"lt_lt\": \"Lithuanian\", \"lv_lv\": \"Latvian\", \"mi_nz\": \"Maori\", \"mk_mk\": \"Macedonian\", \"ml_in\": \"Malayalam\", \"mn_mn\": \"Mongolian\", \"mr_in\": \"Marathi\", \"ms_my\": \"Malay\", \"mt_mt\": \"Maltese\", \"my_mm\": \"Myanmar\", \"nb_no\": \"Norwegian\", \"ne_np\": \"Nepali\", \"nl_nl\": \"Dutch\", \"oc_fr\": \"Occitan\", \"pa_in\": \"Punjabi\", \"pl_pl\": \"Polish\", \"ps_af\": \"Pashto\", \"pt_br\": \"Portuguese\", \"ro_ro\": \"Romanian\", \"ru_ru\": \"Russian\", \"sd_in\": \"Sindhi\", \"sk_sk\": \"Slovak\", \"sl_si\": \"Slovenian\", \"sn_zw\": \"Shona\", \"so_so\": \"Somali\", \"sr_rs\": \"Serbian\", \"sv_se\": \"Swedish\", \"sw_ke\": \"Swahili\", \"ta_in\": \"Tamil\", \"te_in\": \"Telugu\", \"tg_tj\": \"Tajik\", \"th_th\": \"Thai\", \"tr_tr\": \"Turkish\", \"uk_ua\": \"Ukrainian\", \"ur_pk\": \"Urdu\", \"uz_uz\": \"Uzbek\", \"vi_vn\": \"Vietnamese\", \"yo_ng\": \"Yoruba\"}\n",
    "lang = \"en_us\"\n",
    "language = languages[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(language=language, beam_size=5, best_of=5)\n",
    "transcribe_options = dict(task=\"transcribe\", **options)\n",
    "translate_options = dict(task=\"translate\", **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "from dtw import dtw\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import tarfile\n",
    "import whisper\n",
    "import torchaudio\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, target_path: str):\n",
    "    with urllib.request.urlopen(url) as source, open(target_path, \"wb\") as output:\n",
    "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_SAMPLES_PER_TOKEN = whisper.audio.HOP_LENGTH * 2\n",
    "AUDIO_TIME_PER_TOKEN = AUDIO_SAMPLES_PER_TOKEN / whisper.audio.SAMPLE_RATE\n",
    "\n",
    "medfilt_width = 7\n",
    "qk_scale = 1.0\n",
    "\n",
    "tokenizer = get_tokenizer(model.is_multilingual, language=languages[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import WhisperConfig, WhisperModel\n",
    "\n",
    "# # Initializing a Whisper tiny style configuration\n",
    "# configuration = WhisperConfig(vocab_size=50257)\n",
    "# # Initializing a model (with random weights) from the tiny style configuration\n",
    "# model = WhisperModel(configuration)\n",
    "\n",
    "# # Accessing the model configuration\n",
    "# configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "# #50257 tokenizer\n",
    "# # 51865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if languages[lang] in {\"Chinese\", \"Japanese\", \"Korean\"}:\n",
    "    font = \"GoNotoCJKCore.ttf\"\n",
    "else:\n",
    "    font = \"GoNotoCurrent.ttf\"\n",
    "\n",
    "font_release = \"https://github.com/satbyy/go-noto-universal/releases/download/v5.2\"\n",
    "if not os.path.exists(font):\n",
    "    download(f\"{font_release}/{font}\", font)\n",
    "\n",
    "prop = fm.FontProperties(fname=font)\n",
    "props = {'fontproperties': prop}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_on_unicode(tokens: torch.Tensor):\n",
    "    words = []\n",
    "    word_tokens = []\n",
    "    current_tokens = []\n",
    "    \n",
    "    for token in tokens.tolist():\n",
    "        current_tokens.append(token)\n",
    "        decoded = tokenizer.decode_with_timestamps(current_tokens)\n",
    "        if \"\\ufffd\" not in decoded:\n",
    "            words.append(decoded)\n",
    "            word_tokens.append(current_tokens)\n",
    "            current_tokens = []\n",
    "    \n",
    "    return words, word_tokens\n",
    "\n",
    "def split_tokens_on_spaces(tokens: torch.Tensor):\n",
    "    subwords, subword_tokens_list = split_tokens_on_unicode(tokens)\n",
    "    words = []\n",
    "    word_tokens = []\n",
    "    \n",
    "    for subword, subword_tokens in zip(subwords, subword_tokens_list):\n",
    "        special = subword_tokens[0] >= tokenizer.eot\n",
    "        with_space = subword.startswith(\" \")\n",
    "        punctuation = subword.strip() in string.punctuation\n",
    "        if special or with_space or punctuation:\n",
    "            words.append(subword)\n",
    "            word_tokens.append(subword_tokens)\n",
    "        else:\n",
    "            words[-1] = words[-1] + subword\n",
    "            word_tokens[-1].extend(subword_tokens)\n",
    "    \n",
    "    return words, word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if languages[lang] in {\"Chinese\", \"Japanese\", \"Thai\", \"Lao\", \"Myanmar\"}:\n",
    "    # These languages don't typically use spaces, so it is difficult to split words\n",
    "    # without morpheme analysis. Here, we instead split words at any\n",
    "    # position where the tokens are decoded as valid unicode points\n",
    "    split_tokens = split_tokens_on_unicode\n",
    "else:\n",
    "    split_tokens = split_tokens_on_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WhisperModel' object has no attribute 'dims'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# install hooks on the cross attention layers to retrieve the attention weights\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m QKs \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m model\u001b[39m.\u001b[39;49mdims\u001b[39m.\u001b[39mn_text_layer\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mblocks):\n\u001b[0;32m      5\u001b[0m     block\u001b[39m.\u001b[39mcross_attn\u001b[39m.\u001b[39mregister_forward_hook(\n\u001b[0;32m      6\u001b[0m         \u001b[39mlambda\u001b[39;00m _, ins, outs, index\u001b[39m=\u001b[39mi: QKs\u001b[39m.\u001b[39m\u001b[39m__setitem__\u001b[39m(index, outs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m      7\u001b[0m     )\n",
      "File \u001b[1;32md:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WhisperModel' object has no attribute 'dims'"
     ]
    }
   ],
   "source": [
    "# install hooks on the cross attention layers to retrieve the attention weights\n",
    "QKs = [None] * model.dims.n_text_layer\n",
    "\n",
    "for i, block in enumerate(model.decoder.blocks):\n",
    "    block.cross_attn.register_forward_hook(\n",
    "        lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fleurs(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple class to wrap Fleurs and subsample a portion of the dataset as needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, lang, split=\"test\", subsample_rate=1, device=DEVICE):\n",
    "        url = f\"https://storage.googleapis.com/xtreme_translations/FLEURS102/{lang}.tar.gz\"\n",
    "        tar_path = os.path.expanduser(f\"~/.cache/fleurs/{lang}.tgz\")\n",
    "        os.makedirs(os.path.dirname(tar_path), exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(tar_path):\n",
    "            download(url, tar_path)\n",
    "\n",
    "        all_audio = {}\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            for member in tar.getmembers():\n",
    "                name = member.name\n",
    "                if name.endswith(f\"{split}.tsv\"):\n",
    "                    labels = pd.read_table(tar.extractfile(member), names=(\"id\", \"file_name\", \"raw_transcription\", \"transcription\", \"_\", \"num_samples\", \"gender\"))\n",
    "\n",
    "                if f\"/{split}/\" in name and name.endswith(\".wav\"):\n",
    "                    audio_bytes = tar.extractfile(member).read()\n",
    "                    all_audio[os.path.basename(name)] = wavfile.read(io.BytesIO(audio_bytes))[1]                    \n",
    "\n",
    "        self.labels = labels.to_dict(\"records\")[::subsample_rate]\n",
    "        self.all_audio = all_audio\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        record = self.labels[item]\n",
    "        audio = torch.from_numpy(self.all_audio[record[\"file_name\"]].copy())\n",
    "        text = record[\"transcription\"]\n",
    "        \n",
    "        return (audio, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Fleurs(lang, subsample_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WhisperModel' object has no attribute 'detect_language'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# for the first 10 examples in the dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# for (audio, label), transcription in zip(dataset, transcriptions[:10]):\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m#     # print(audio)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m#     print('here')\u001b[39;00m\n\u001b[0;32m     13\u001b[0m audio_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata/audio versiunea 1.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m transcription \u001b[39m=\u001b[39m audio_to_text(audio_file)\n\u001b[0;32m     17\u001b[0m audio_bytes \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(audio_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mread()\n\u001b[0;32m     18\u001b[0m \u001b[39m# audio = wavfile.read(io.BytesIO(audio_bytes))[1]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# print(audio_bytes)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36maudio_to_text\u001b[1;34m(audio)\u001b[0m\n\u001b[0;32m     20\u001b[0m audio \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39mpad_or_trim(audio)\n\u001b[0;32m     21\u001b[0m mel \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39mlog_mel_spectrogram(audio)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 22\u001b[0m _, probs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdetect_language(mel)\n\u001b[0;32m     24\u001b[0m detected_language \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(probs, key\u001b[39m=\u001b[39mprobs\u001b[39m.\u001b[39mget)\n\u001b[0;32m     25\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtranscribe(audio)\n",
      "File \u001b[1;32md:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WhisperModel' object has no attribute 'detect_language'"
     ]
    }
   ],
   "source": [
    "# for the first 10 examples in the dataset\n",
    "# for (audio, label), transcription in zip(dataset, transcriptions[:10]):\n",
    "\n",
    "# audio\n",
    "# transcription\n",
    "# import wave\n",
    "\n",
    "# with wave.open('data/example.wav', 'rb') as wav_file:\n",
    "#     audio = wavfile.read(io.BytesIO(wav_file))[1]\n",
    "#     # print(audio)\n",
    "#     print('here')\n",
    "\n",
    "audio_file = \"data/audio versiunea 1.wav\"\n",
    "\n",
    "transcription = audio_to_text(audio_file)\n",
    "\n",
    "audio_bytes = open(audio_file, \"rb\").read()\n",
    "# audio = wavfile.read(io.BytesIO(audio_bytes))[1]\n",
    "# print(audio_bytes)\n",
    "audio = torch.from_numpy(wavfile.read(io.BytesIO(audio_bytes))[1]).float().to(DEVICE)\n",
    "# print(audio.)\n",
    "# print(audio)\n",
    "\n",
    "print(transcription)\n",
    "\n",
    "duration = len(audio)\n",
    "mel = whisper.log_mel_spectrogram(whisper.pad_or_trim(audio)).to(DEVICE)\n",
    "tokens = torch.tensor(\n",
    "    [\n",
    "        *tokenizer.sot_sequence,\n",
    "        tokenizer.timestamp_begin,\n",
    "    ] + tokenizer.encode(transcription) + [\n",
    "        tokenizer.timestamp_begin + duration // AUDIO_SAMPLES_PER_TOKEN,\n",
    "        tokenizer.eot,\n",
    "    ]\n",
    ").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))\n",
    "\n",
    "weights = torch.cat(QKs)  # layers * heads * tokens * frames    \n",
    "weights = weights[:, :, :, : duration // AUDIO_SAMPLES_PER_TOKEN].cpu()\n",
    "weights = median_filter(weights, (1, 1, 1, medfilt_width))\n",
    "weights = torch.tensor(weights * qk_scale).softmax(dim=-1)\n",
    "\n",
    "w = weights / weights.norm(dim=-2, keepdim=True)\n",
    "matrix = w[-6:].mean(axis=(0, 1))\n",
    "\n",
    "alignment = dtw(-matrix.double().numpy())\n",
    "\n",
    "jumps = np.pad(np.diff(alignment.index1s), (1, 0), constant_values=1).astype(bool)\n",
    "jump_times = alignment.index2s[jumps] * AUDIO_TIME_PER_TOKEN\n",
    "words, word_tokens = split_tokens(tokens)\n",
    "\n",
    "# display the normalized attention weights and the alignment\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(matrix, aspect=\"auto\")\n",
    "plt.plot(alignment.index2s, alignment.index1s, color=\"red\")\n",
    "\n",
    "xticks = np.arange(0, matrix.shape[1], 1 / AUDIO_TIME_PER_TOKEN)\n",
    "xticklabels = (xticks * AUDIO_TIME_PER_TOKEN).round().astype(np.int32) \n",
    "plt.xticks(xticks, xticklabels)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# display tokens and words as tick labels\n",
    "ylims = plt.gca().get_ylim()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.tick_params('both', length=0, width=0, which='minor', pad=6)\n",
    "\n",
    "ax.yaxis.set_ticks_position(\"left\")\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "ax.invert_yaxis()\n",
    "ax.set_ylim(ylims)\n",
    "\n",
    "major_ticks = [-0.5]\n",
    "minor_ticks = []\n",
    "current_y = 0\n",
    "\n",
    "for word, word_token in zip(words, word_tokens):\n",
    "    minor_ticks.append(current_y + len(word_token) / 2 - 0.5)\n",
    "    current_y += len(word_token)\n",
    "    major_ticks.append(current_y - 0.5)\n",
    "    \n",
    "ax.yaxis.set_minor_locator(ticker.FixedLocator(minor_ticks))\n",
    "ax.yaxis.set_minor_formatter(ticker.FixedFormatter(words))\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "\n",
    "for label in ax.get_yminorticklabels():\n",
    "    label.set_fontproperties(prop)\n",
    "\n",
    "plt.ylabel(\"Words\")\n",
    "plt.show()\n",
    "\n",
    "# display the word-level timestamps in a table\n",
    "word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n",
    "begin_times = jump_times[word_boundaries[:-1]]\n",
    "end_times = jump_times[word_boundaries[1:]]\n",
    "\n",
    "data = [\n",
    "    dict(word=word, begin=begin, end=end)\n",
    "    for word, begin, end in zip(words[:-1], begin_times, end_times)\n",
    "    if not word.startswith(\"<|\") and word.strip() not in \".,!?、。\"\n",
    "]\n",
    "\n",
    "display(pd.DataFrame(data))\n",
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f4fa1dd9812ef84c6f62fa4d0eb9af974f88682b41fada47f03f45295ccfca9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
