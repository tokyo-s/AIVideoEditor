{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ffmpeg?????????????? ffmpeg-python lib?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dtw-python\n",
      "  Downloading dtw_python-1.3.0-cp310-cp310-win_amd64.whl (303 kB)\n",
      "Requirement already satisfied: numpy>=1.19 in d:\\programming\\ml\\univer\\teza\\env\\lib\\site-packages (from dtw-python) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.1 in d:\\programming\\ml\\univer\\teza\\env\\lib\\site-packages (from dtw-python) (1.10.0)\n",
      "Installing collected packages: dtw-python\n",
      "Successfully installed dtw-python-1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'D:\\programming\\ML\\univer\\TEZA\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install scipy\n",
    "# ! pip install dtw-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 461M/461M [00:15<00:00, 31.1MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "from dtw import dtw\n",
    "from scipy.ndimage import median_filter\n",
    "import numpy as np\n",
    "\n",
    "MODEL_WHISPER_SIZE = \"small\" #large\n",
    "audio_file = \"data/audio versiunea 1.mp3\"\n",
    "\n",
    "model = whisper.load_model(MODEL_WHISPER_SIZE, device=\"cpu\")\n",
    "\n",
    "def audio_to_text(audio):\n",
    "    audio = whisper.load_audio(audio)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "    _, probs = model.detect_language(mel)\n",
    "\n",
    "    detected_language = max(probs, key=probs.get)\n",
    "    result = model.transcribe(audio)\n",
    "    text = result[\"text\"]\n",
    "    return detected_language, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dtw module. When using in academic works please cite:\n",
    "#   T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
    "#   J. Stat. Soft., doi:10.18637/jss.v031.i07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch -y\n",
    "# !pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now need to get timestamps for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.tokenizer import get_tokenizer\n",
    "import string \n",
    "\n",
    "AUDIO_SAMPLES_PER_TOKEN = whisper.audio.HOP_LENGTH * 2\n",
    "AUDIO_TIME_PER_TOKEN = AUDIO_SAMPLES_PER_TOKEN / whisper.audio.SAMPLE_RATE\n",
    "medfilt_width = 7\n",
    "qk_scale = 1.0\n",
    "languages = {\"af_za\": \"Afrikaans\", \"am_et\": \"Amharic\", \"ar_eg\": \"Arabic\", \"as_in\": \"Assamese\", \"az_az\": \"Azerbaijani\", \"be_by\": \"Belarusian\", \"bg_bg\": \"Bulgarian\", \"bn_in\": \"Bengali\", \"bs_ba\": \"Bosnian\", \"ca_es\": \"Catalan\", \"cmn_hans_cn\": \"Chinese\", \"cs_cz\": \"Czech\", \"cy_gb\": \"Welsh\", \"da_dk\": \"Danish\", \"de_de\": \"German\", \"el_gr\": \"Greek\", \"en_us\": \"English\", \"es_419\": \"Spanish\", \"et_ee\": \"Estonian\", \"fa_ir\": \"Persian\", \"fi_fi\": \"Finnish\", \"fil_ph\": \"Tagalog\", \"fr_fr\": \"French\", \"gl_es\": \"Galician\", \"gu_in\": \"Gujarati\", \"ha_ng\": \"Hausa\", \"he_il\": \"Hebrew\", \"hi_in\": \"Hindi\", \"hr_hr\": \"Croatian\", \"hu_hu\": \"Hungarian\", \"hy_am\": \"Armenian\", \"id_id\": \"Indonesian\", \"is_is\": \"Icelandic\", \"it_it\": \"Italian\", \"ja_jp\": \"Japanese\", \"jv_id\": \"Javanese\", \"ka_ge\": \"Georgian\", \"kk_kz\": \"Kazakh\", \"km_kh\": \"Khmer\", \"kn_in\": \"Kannada\", \"ko_kr\": \"Korean\", \"lb_lu\": \"Luxembourgish\", \"ln_cd\": \"Lingala\", \"lo_la\": \"Lao\", \"lt_lt\": \"Lithuanian\", \"lv_lv\": \"Latvian\", \"mi_nz\": \"Maori\", \"mk_mk\": \"Macedonian\", \"ml_in\": \"Malayalam\", \"mn_mn\": \"Mongolian\", \"mr_in\": \"Marathi\", \"ms_my\": \"Malay\", \"mt_mt\": \"Maltese\", \"my_mm\": \"Myanmar\", \"nb_no\": \"Norwegian\", \"ne_np\": \"Nepali\", \"nl_nl\": \"Dutch\", \"oc_fr\": \"Occitan\", \"pa_in\": \"Punjabi\", \"pl_pl\": \"Polish\", \"ps_af\": \"Pashto\", \"pt_br\": \"Portuguese\", \"ro_ro\": \"Romanian\", \"ru_ru\": \"Russian\", \"sd_in\": \"Sindhi\", \"sk_sk\": \"Slovak\", \"sl_si\": \"Slovenian\", \"sn_zw\": \"Shona\", \"so_so\": \"Somali\", \"sr_rs\": \"Serbian\", \"sv_se\": \"Swedish\", \"sw_ke\": \"Swahili\", \"ta_in\": \"Tamil\", \"te_in\": \"Telugu\", \"tg_tj\": \"Tajik\", \"th_th\": \"Thai\", \"tr_tr\": \"Turkish\", \"uk_ua\": \"Ukrainian\", \"ur_pk\": \"Urdu\", \"uz_uz\": \"Uzbek\", \"vi_vn\": \"Vietnamese\", \"yo_ng\": \"Yoruba\"}\n",
    "selected_language = \"en_us\"\n",
    "tokenizer = get_tokenizer(model.is_multilingual, language=languages[selected_language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_on_unicode(tokens):\n",
    "    words = []\n",
    "    word_tokens = []\n",
    "    current_tokens = []\n",
    "    \n",
    "    for token in tokens.tolist():\n",
    "        current_tokens.append(token)\n",
    "        decoded = tokenizer.decode_with_timestamps(current_tokens)\n",
    "        if \"\\ufffd\" not in decoded:\n",
    "            words.append(decoded)\n",
    "            word_tokens.append(current_tokens)\n",
    "            current_tokens = []\n",
    "    \n",
    "    return words, word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_tokens_on_spaces(tokens):\n",
    "    subwords, subword_tokens_list = split_tokens_on_unicode(tokens)\n",
    "    words = []\n",
    "    word_tokens = []\n",
    "    \n",
    "    for subword, subword_tokens in zip(subwords, subword_tokens_list):\n",
    "        special = subword_tokens[0] >= tokenizer.eot\n",
    "        with_space = subword.startswith(\" \")\n",
    "        punctuation = subword.strip() in string.punctuation\n",
    "        if special or with_space or punctuation:\n",
    "            words.append(subword)\n",
    "            word_tokens.append(subword_tokens)\n",
    "        else:\n",
    "            words[-1] = words[-1] + subword\n",
    "            word_tokens[-1].extend(subword_tokens)\n",
    "    \n",
    "    return words, word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if languages[selected_language] in {\"Chinese\", \"Japanese\", \"Thai\", \"Lao\", \"Myanmar\"}:\n",
    "    # These languages don't typically use spaces, so it is difficult to split words\n",
    "    # without morpheme analysis. Here, we instead split words at any\n",
    "    # position where the tokens are decoded as valid unicode points\n",
    "    split_tokens = split_tokens_on_unicode\n",
    "else:\n",
    "    split_tokens = split_tokens_on_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install hooks on the cross attention layers to retrieve the attention weights\n",
    "QKs = [None] * model.dims.n_text_layer\n",
    "\n",
    "for i, block in enumerate(model.decoder.blocks):\n",
    "    block.cross_attn.register_forward_hook(\n",
    "        lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\whisper\\transcribe.py:77: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "d:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\whisper\\transcribe.py:79: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ro',\n",
       " ' Personajul narrator din franceza Draza in tipologia conservatorului posturile din jurul sau se schimbă din ce cauza, de ce aici că trebuia să predea ca a sa unul mă simteam bine aici, eram singură, eram lăsat în pace, se întoarce ca să apărintească când era bine a s-a cu plăcere să am o cameră, le-am acaută caracterizare, pe personajului datorită negarea unul care nu putți spune despre mine că sunt prea gras prin autoevaluarea comportamentului său.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription= audio_to_text(audio_file)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import io\n",
    "# from scipy.io import wavfile\n",
    "# import tarfile\n",
    "# import urllib\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def download(url: str, target_path: str):\n",
    "#     with urllib.request.urlopen(url) as source, open(target_path, \"wb\") as output:\n",
    "#         with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
    "#             while True:\n",
    "#                 buffer = source.read(8192)\n",
    "#                 if not buffer:\n",
    "#                     break\n",
    "\n",
    "#                 output.write(buffer)\n",
    "#                 loop.update(len(buffer))\n",
    "\n",
    "# class Fleurs(torch.utils.data.Dataset):\n",
    "#     \"\"\"\n",
    "#     A simple class to wrap Fleurs and subsample a portion of the dataset as needed.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, lang, split=\"test\", subsample_rate=1, device='cpu'):\n",
    "#         url = f\"https://storage.googleapis.com/xtreme_translations/FLEURS102/{lang}.tar.gz\"\n",
    "#         tar_path = os.path.expanduser(f\"~/.cache/fleurs/{lang}.tgz\")\n",
    "#         os.makedirs(os.path.dirname(tar_path), exist_ok=True)\n",
    "\n",
    "#         if not os.path.exists(tar_path):\n",
    "#             download(url, tar_path)\n",
    "\n",
    "#         all_audio = {}\n",
    "#         with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "#             for member in tar.getmembers():\n",
    "#                 name = member.name\n",
    "#                 if name.endswith(f\"{split}.tsv\"):\n",
    "#                     labels = pd.read_table(tar.extractfile(member), names=(\"id\", \"file_name\", \"raw_transcription\", \"transcription\", \"_\", \"num_samples\", \"gender\"))\n",
    "\n",
    "#                 if f\"/{split}/\" in name and name.endswith(\".wav\"):\n",
    "#                     audio_bytes = tar.extractfile(member).read()\n",
    "#                     all_audio[os.path.basename(name)] = wavfile.read(io.BytesIO(audio_bytes))[1]                    \n",
    "\n",
    "#         self.labels = labels.to_dict(\"records\")[::subsample_rate]\n",
    "#         self.all_audio = all_audio\n",
    "#         self.device = device\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         record = self.labels[item]\n",
    "#         audio = torch.from_numpy(self.all_audio[record[\"file_name\"]].copy())\n",
    "#         text = record[\"transcription\"]\n",
    "        \n",
    "#         return (audio, text)\n",
    "\n",
    "# dataset = Fleurs(selected_language, subsample_rate=10)  # subsample 10% of the dataset for a quick demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048660\n",
      "torch.Size([80, 3000])\n",
      "torch.Size([164])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(tokens\u001b[39m.\u001b[39msize())\n\u001b[0;32m     19\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 20\u001b[0m     logits \u001b[39m=\u001b[39m model(mel\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), tokens\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[0;32m     22\u001b[0m weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(QKs)  \u001b[39m# layers * heads * tokens * frames    \u001b[39;00m\n\u001b[0;32m     23\u001b[0m weights \u001b[39m=\u001b[39m weights[:, :, :, : duration \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m AUDIO_SAMPLES_PER_TOKEN]\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[1;32md:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\whisper\\model.py:224\u001b[0m, in \u001b[0;36mWhisper.forward\u001b[1;34m(self, mel, tokens)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, mel: torch\u001b[39m.\u001b[39mTensor, tokens: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(tokens, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(mel))\n",
      "File \u001b[1;32md:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\whisper\\model.py:186\u001b[0m, in \u001b[0;36mTextDecoder.forward\u001b[1;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39mx : torch.LongTensor, shape = (batch_size, <= n_ctx)\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m    the text tokens\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39mxa : torch.Tensor, shape = (batch_size, n_mels, n_audio_ctx)\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m    the encoded audio features to be attended on\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m offset \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kv_cache\u001b[39m.\u001b[39mvalues()))\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m kv_cache \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 186\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken_embedding(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding[offset : offset \u001b[39m+\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[0;32m    187\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(xa\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    189\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n",
      "File \u001b[1;32md:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32md:\\programming\\ML\\univer\\TEZA\\env\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# for the first 10 examples in the dataset\n",
    "# for (audio, label), transcription in zip(dataset, transcriptions[:10]):\n",
    "\n",
    "audio = whisper.load_audio(audio_file)\n",
    "duration = len(audio)\n",
    "print(duration)\n",
    "mel = whisper.log_mel_spectrogram(whisper.pad_or_trim(audio)).cpu()\n",
    "tokens = torch.tensor(\n",
    "    [\n",
    "        *tokenizer.sot_sequence,\n",
    "        tokenizer.timestamp_begin,\n",
    "    ] + tokenizer.encode(transcription) + [\n",
    "        tokenizer.timestamp_begin + duration // AUDIO_SAMPLES_PER_TOKEN,\n",
    "        tokenizer.eot,\n",
    "    ]\n",
    ").cpu()\n",
    "print(mel.size())\n",
    "print(tokens.size())\n",
    "with torch.no_grad():\n",
    "    logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))\n",
    "\n",
    "weights = torch.cat(QKs)  # layers * heads * tokens * frames    \n",
    "weights = weights[:, :, :, : duration // AUDIO_SAMPLES_PER_TOKEN].cpu()\n",
    "weights = median_filter(weights, (1, 1, 1, medfilt_width))\n",
    "weights = torch.tensor(weights * qk_scale).softmax(dim=-1)\n",
    "\n",
    "w = weights / weights.norm(dim=-2, keepdim=True)\n",
    "matrix = w[-6:].mean(axis=(0, 1))\n",
    "\n",
    "alignment = dtw(-matrix.double().numpy())\n",
    "\n",
    "jumps = np.pad(np.diff(alignment.index1s), (1, 0), constant_values=1).astype(bool)\n",
    "jump_times = alignment.index2s[jumps] * AUDIO_TIME_PER_TOKEN\n",
    "words, word_tokens = split_tokens(tokens)\n",
    "\n",
    "# display the normalized attention weights and the alignment\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(matrix, aspect=\"auto\")\n",
    "plt.plot(alignment.index2s, alignment.index1s, color=\"red\")\n",
    "\n",
    "xticks = np.arange(0, matrix.shape[1], 1 / AUDIO_TIME_PER_TOKEN)\n",
    "xticklabels = (xticks * AUDIO_TIME_PER_TOKEN).round().astype(np.int32) \n",
    "plt.xticks(xticks, xticklabels)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# display tokens and words as tick labels\n",
    "ylims = plt.gca().get_ylim()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.tick_params('both', length=0, width=0, which='minor', pad=6)\n",
    "\n",
    "ax.yaxis.set_ticks_position(\"left\")\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "ax.invert_yaxis()\n",
    "ax.set_ylim(ylims)\n",
    "\n",
    "major_ticks = [-0.5]\n",
    "minor_ticks = []\n",
    "current_y = 0\n",
    "\n",
    "for word, word_token in zip(words, word_tokens):\n",
    "    minor_ticks.append(current_y + len(word_token) / 2 - 0.5)\n",
    "    current_y += len(word_token)\n",
    "    major_ticks.append(current_y - 0.5)\n",
    "    \n",
    "ax.yaxis.set_minor_locator(ticker.FixedLocator(minor_ticks))\n",
    "ax.yaxis.set_minor_formatter(ticker.FixedFormatter(words))\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "\n",
    "# for label in ax.get_yminorticklabels():\n",
    "#     label.set_fontproperties(prop)\n",
    "\n",
    "plt.ylabel(\"Words\")\n",
    "plt.show()\n",
    "\n",
    "# display the word-level timestamps in a table\n",
    "word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n",
    "begin_times = jump_times[word_boundaries[:-1]]\n",
    "end_times = jump_times[word_boundaries[1:]]\n",
    "\n",
    "data = [\n",
    "    dict(word=word, begin=begin, end=end)\n",
    "    for word, begin, end in zip(words[:-1], begin_times, end_times)\n",
    "    if not word.startswith(\"<|\") and word.strip() not in \".,!?、。\"\n",
    "]\n",
    "\n",
    "display(pd.DataFrame(data))\n",
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f4fa1dd9812ef84c6f62fa4d0eb9af974f88682b41fada47f03f45295ccfca9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
